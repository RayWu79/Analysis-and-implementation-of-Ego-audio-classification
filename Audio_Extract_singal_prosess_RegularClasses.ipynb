{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4624e43b",
   "metadata": {},
   "source": [
    "## Audio Signal Extract and Process to Regular Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050ed81",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1842e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing library for analyzing and extracting features from audio signals\n",
    "import librosa\n",
    "\n",
    "# Library for reducing noise from audio signals\n",
    "import noisereduce as nr\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Library for working with video files, including reading and editing\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "from numpy import isfinite \n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b880d",
   "metadata": {},
   "source": [
    "### Require Data Loading and Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bddb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chijuiwu/snap/snapd-desktop-integration/83/Desktop/CapstoneProject\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11948328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of training excal which including video name, labels, duration, etc.\n",
    "Train_excal_path = '/home/chijuiwu/snap/snapd-desktop-integration/83/Desktop/CapstoneProject/CharadesEgoInfo/CharadesEgo_v1_test_only1st.csv'\n",
    "\n",
    "df = pd.read_csv(Train_excal_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d413762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the content in the csv. file\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734db708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only 'id' and 'actions' columns\n",
    "selected_data = df[['id', 'actions']]\n",
    "\n",
    "print(selected_data)\n",
    "\n",
    "# The 'actions' category contain the action and second. \n",
    "# For instance, c115 is the action 'Someone is holding a paper/notebook'.\n",
    "# The first 0.00 is the start sec of the action, The second 21.70 is the end sec of the action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique action codes \n",
    "action_pattern = r'c\\d{3}'\n",
    "unique_actions = set()\n",
    "\n",
    "unique_actions_with_no_action = set(['no_action'])  # Initialize set with 'no_action' for NaN cases\n",
    "\n",
    "for actions in df['actions']:\n",
    "    if pd.isna(actions): # Check if the actions data is NaN (missing), and skip further processing for this row.\n",
    "        continue  \n",
    "    matches = re.findall(action_pattern, str(actions)) # Use regex to find all occurrences of the action pattern in the actions data.\n",
    "    unique_actions_with_no_action.update(matches) # Update the set with any new action codes found in the current row.\n",
    "\n",
    "# Sorting the unique_actions set\n",
    "sorted_unique_actions = sorted(unique_actions_with_no_action)\n",
    "print(sorted_unique_actions)\n",
    "\n",
    "# Counting the number of sorted unique actions\n",
    "number_of_actions = len(sorted_unique_actions)\n",
    "print(f\"number of actions: {number_of_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'id' column to list for video_files.\n",
    "# The name of each vedio file for extract audio process.\n",
    "video_files = selected_data['id'].tolist()\n",
    "print(video_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from 'id' and 'actions' for annotations\n",
    "annotations = dict(zip(selected_data['id'], selected_data['actions']))\n",
    "print(annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2c508",
   "metadata": {},
   "source": [
    "## Functions for Extract Audio and Label the Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse annotations\n",
    "def parse_annotations(annotation_str, video_duration):\n",
    "    actions = []\n",
    "    last_end = 0\n",
    "\n",
    "    # Check if annotation_str is a float and nan\n",
    "    if isinstance(annotation_str, float) and math.isnan(annotation_str):\n",
    "        # Handle the case where the annotation is nan\n",
    "        return [{'action': 'no_action', 'start': 0, 'end': video_duration}]\n",
    "\n",
    "    # Proceed if annotation_str is not nan\n",
    "    for action in annotation_str.split(';'):\n",
    "        parts = action.split()\n",
    "        action_id = parts[0]\n",
    "        start = round(float(parts[1]))\n",
    "        end = round(float(parts[2]))\n",
    "\n",
    "        # Check for gap between last action and current action\n",
    "        if start > last_end:\n",
    "            actions.append({'action': 'no_action', 'start': last_end, 'end': start})\n",
    "\n",
    "        actions.append({'action': action_id, 'start': start, 'end': end})\n",
    "        last_end = end\n",
    "\n",
    "    # Check for action-less segment at the end of the video\n",
    "    if last_end < video_duration:\n",
    "        actions.append({'action': 'no_action', 'start': last_end, 'end': video_duration})\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "def process_video(file_name, annotations, file_path):\n",
    "    \n",
    "    # Load the video by file name and video's duration\n",
    "    video = VideoFileClip(f\"{file_path}/{file_name}.mp4\")\n",
    "    duration = int(video.duration)\n",
    "    \n",
    "    #Initial index for mfcc and spectrogram\n",
    "    mfcc_data = []\n",
    "    spectrogram_data = []\n",
    "    sample_rate = 16000\n",
    "\n",
    "    for i in range(duration):\n",
    "        audio_clip = video.subclip(i, i + 1).audio\n",
    "        audio_clip_filename = f\"audio_clip_{i}.wav\"\n",
    "        audio_clip.write_audiofile(audio_clip_filename, codec='pcm_s16le')\n",
    "\n",
    "        audio_clip_input, sr = librosa.load(audio_clip_filename, sr=sample_rate)\n",
    "        \n",
    "        # Check and handle non-finite values in the audio clip\n",
    "        if not np.isfinite(audio_clip_input).all():\n",
    "            non_finite_indices = np.where(~np.isfinite(audio_clip_input))\n",
    "            print(f\"Non-finite values found in audio clip {i} of {file_name} at indices {non_finite_indices}, handling them.\")\n",
    "            audio_clip_input = np.nan_to_num(audio_clip_input)  # Replace NaN and Inf with zero\n",
    "\n",
    "        audio_clip_input_reduced = nr.reduce_noise(y=audio_clip_input, sr=sample_rate)\n",
    "\n",
    "        # Remove the temporary audio file, saving storage space\n",
    "        os.remove(audio_clip_filename)\n",
    "\n",
    "        # Check for non-finite values\n",
    "        if not np.isfinite(audio_clip_input).all():\n",
    "            print(f\"Non-finite values found in audio clip {i} of {file_name}, skipping this clip.\")\n",
    "            continue\n",
    "\n",
    "        # Perform noise reduction\n",
    "        try:\n",
    "            audio_clip_input_reduced = nr.reduce_noise(y=audio_clip_input, sr=sr)\n",
    "        # There are some vedio duration is less then one sec, skippig the video\n",
    "        except Exception as e:\n",
    "            print(f\"Error in noise reduction for clip {i} of {file_name}: {e}, skipping this clip.\")\n",
    "            continue\n",
    "\n",
    "        # Check again for non-finite values after noise reduction\n",
    "        if not np.isfinite(audio_clip_input_reduced).all():\n",
    "            print(f\"Non-finite values found after noise reduction in audio clip {i} of {file_name}, skipping this clip.\")\n",
    "            continue\n",
    "            \n",
    "        # Generate mfcca and spectrogram by librosa\n",
    "        mfcc = librosa.feature.mfcc(y=audio_clip_input_reduced, sr=sample_rate)\n",
    "        spect = librosa.amplitude_to_db(np.abs(librosa.stft(audio_clip_input_reduced)), ref=np.max)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Resizing operations to fit the model input requirement\n",
    "        # Since I only use the spectrogram to training the model, therefore I only reshape the spectrogram data\n",
    "        # If use the mfcc to the model, can use the similar method to fit the model input requirement\n",
    "        \n",
    "        # Adjust frequency dimension to 1024\n",
    "        if spect.shape[0] > 1024:\n",
    "            spect = spect[:1024, :]\n",
    "        elif spect.shape[0] < 1024:\n",
    "            padding_size = 1024 - spect.shape[0]\n",
    "            spect = np.pad(spect, ((0, padding_size), (0, 0)))\n",
    "\n",
    "        # Convert to tensor for resampling\n",
    "        spect_tensor = torch.tensor(spect, dtype=torch.float32)\n",
    "\n",
    "        # Add a channel dimension for interpolation\n",
    "        spect_tensor = spect_tensor.unsqueeze(0).unsqueeze(0)  # Now [1, 1, 1024, Time]\n",
    "\n",
    "        # Resample time dimension to 128\n",
    "        spect_tensor = F.interpolate(spect_tensor, size=(1024, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Remove channel dimension\n",
    "        spect = spect_tensor.squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "        # Identify all actions that occur in this second\n",
    "        actions_in_this_second = []\n",
    "        for action in annotations:\n",
    "            if i >= action['start'] and i < action['end']:\n",
    "                actions_in_this_second.append(action['action'])\n",
    "\n",
    "        # If no specific action, label this as 'no_action'\n",
    "        if not actions_in_this_second:\n",
    "            actions_in_this_second.append('no_action')\n",
    "\n",
    "        # Create a separate entry for each action\n",
    "        for action in actions_in_this_second:\n",
    "            mfcc_data.append({\n",
    "                'file_id': file_name,\n",
    "                'sample_rate': sample_rate,\n",
    "                'start_sec': i,\n",
    "                'end_sec': i + 1,\n",
    "                'mfcc': mfcc,\n",
    "                'action': action\n",
    "            })\n",
    "\n",
    "            spectrogram_data.append({\n",
    "                'file_id': file_name,\n",
    "                'sample_rate': sample_rate,\n",
    "                'start_sec': i,\n",
    "                'end_sec': i + 1,\n",
    "                'spectrogram': spect,\n",
    "                'action': action\n",
    "            })\n",
    "\n",
    "    return mfcc_data, spectrogram_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab007b",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of video file\n",
    "video_file_path = \"/media/DiskDrive1/CharadesEgo\"\n",
    "\n",
    "\n",
    "# Main processing loop\n",
    "all_mfcc_data = []\n",
    "all_spectrogram_data = []\n",
    "\n",
    "for file in video_files:\n",
    "    video = VideoFileClip(f\"{video_file_path}/{file}.mp4\")\n",
    "    parsed_anno = parse_annotations(annotations[file], int(video.duration))\n",
    "\n",
    "    mfcc_data, spectrogram_data = process_video(file, parsed_anno, video_file_path)\n",
    "    all_mfcc_data.extend(mfcc_data)\n",
    "    all_spectrogram_data.extend(spectrogram_data)\n",
    "\n",
    "# Convert to DataFrame and then to dictionaries\n",
    "mfcc_dataset = pd.DataFrame(all_mfcc_data).to_dict('records')\n",
    "spectrogram_dataset = pd.DataFrame(all_spectrogram_data).to_dict('records')\n",
    "\n",
    "# Save the datasets\n",
    "with open('mfcc_dataset.pkl', 'wb') as mfcc_file:\n",
    "    pickle.dump(mfcc_dataset, mfcc_file)\n",
    "\n",
    "with open('spectrogram_dataset.pkl', 'wb') as spectrogram_file:\n",
    "    pickle.dump(spectrogram_dataset, spectrogram_file)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"MFCC Dataset num_rows: {len(mfcc_dataset)}\")\n",
    "print(f\"Spectrogram Dataset num_rows: {len(spectrogram_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60edc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data format\n",
    "print(spectrogram_dataset[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb9af8",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b733d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize MFCC dataset\n",
    "for record in mfcc_dataset:\n",
    "    record['mfcc'] = (record['mfcc'] - (-4.2677393)) / (4.5689974 * 2)\n",
    "\n",
    "# Normalize Spectrogram dataset\n",
    "for record in spectrogram_dataset:\n",
    "    record['spectrogram'] = (record['spectrogram'] - (-4.2677393)) / (4.5689974 * 2)\n",
    "    \n",
    "# Save the datasets\n",
    "with open('mfcc_dataset.pkl', 'wb') as mfcc_file:\n",
    "    pickle.dump(mfcc_dataset, mfcc_file)\n",
    "\n",
    "with open('spectrogram_dataset.pkl', 'wb') as spectrogram_file:\n",
    "    pickle.dump(spectrogram_dataset, spectrogram_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7402f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('spectrogram_dataset.pkl', 'rb') as file:\n",
    "    # Load the object from the pickle file\n",
    "    spectrogram_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5136e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def save_zipped_pickle_gz(obj, filename, protocol=-1):\n",
    "    # Using gzip to compress the data\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol)\n",
    "\n",
    "save_zipped_pickle_gz(spectrogram_dataset, 'spectrogram_dataset.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8f4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
